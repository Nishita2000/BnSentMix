{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8583774,"sourceType":"datasetVersion","datasetId":5133690}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Enable CUDA_LAUNCH_BLOCKING for better error messages\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length, num_labels):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.num_labels = num_labels\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        sentence = str(self.data.iloc[index]['sentence1'])  # Assuming 'sentence1' is the column name\n        label = self.data.iloc[index]['label']  # Get label value\n        \n        # Check if label is a valid number\n        if pd.isnull(label):\n            print(f\"NaN value found in 'label' column at index: {index}\")\n            label = 0\n        else:\n            label = int(label)\n        \n        # Ensure label is within the valid range\n        if label < 0 or label >= self.num_labels:\n            print(f\"Invalid label {label} at index: {index}\")\n            label = 0\n        \n        encoding = self.tokenizer.encode_plus(\n            sentence,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n            truncation=True\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Load tokenizer and model\nnum_labels = 4\ntokenizer = AutoTokenizer.from_pretrained(\"aplycaebous/tb-BERT-fpt\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"aplycaebous/tb-BERT-fpt\", num_labels=num_labels).to(device)\n\n# Load datasets\ntrain_data = pd.read_csv('/kaggle/input/sentiment01/train_data1.csv')  # Update with your file path\nval_data = pd.read_csv('/kaggle/input/sentiment01/validation_data1.csv')  # Update with your file path\ntest_data = pd.read_csv('/kaggle/input/sentiment01/test_data1.csv')  # Update with your file path\n\n# Define datasets and dataloaders\ntrain_dataset = CustomDataset(train_data, tokenizer, max_length=64, num_labels=num_labels)  # Update max_length as needed\nval_dataset = CustomDataset(val_data, tokenizer, max_length=64, num_labels=num_labels)\ntest_dataset = CustomDataset(test_data, tokenizer, max_length=64, num_labels=num_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n# Define optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=1.25e-6)\nepochs = 15\ntotal_steps = len(train_loader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Initialize lists to store training and validation accuracies\ntrain_accuracies = []\nval_accuracies = []\n\n# Training loop\nfor epoch in range(epochs):\n    model.train()\n    total_train_loss = 0\n    correct_train_preds = 0\n    total_train_preds = 0\n    \n    for batch in train_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        total_train_loss += loss.item()\n        \n        preds = torch.argmax(outputs.logits, dim=1)\n        correct_train_preds += (preds == labels).sum().item()\n        total_train_preds += labels.size(0)\n        \n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_accuracy = correct_train_preds / total_train_preds\n    train_accuracies.append(train_accuracy)\n    \n    # Evaluation on validation set\n    model.eval()\n    val_preds = []\n    val_labels = []\n    for batch in val_loader:\n        with torch.no_grad():\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n\n            val_preds.extend(preds.tolist())\n            val_labels.extend(labels.tolist())\n\n    val_accuracy = accuracy_score(val_labels, val_preds)\n    val_accuracies.append(val_accuracy)\n    \n    precision, recall, f1, _ = precision_recall_fscore_support(val_labels, val_preds, average='weighted')\n    \n    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\n# Plot training vs validation accuracy\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, epochs + 1), train_accuracies, label='Train Accuracy')\nplt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Train Accuracy vs Validation Accuracy')\nplt.legend()\nplt.grid()\nplt.show()\n\n# Evaluation on test set\ntest_preds = []\ntest_labels = []\nfor batch in test_loader:\n    with torch.no_grad():\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        \n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=1)\n        \n        test_preds.extend(preds.tolist())\n        test_labels.extend(labels.tolist())\n\ntest_accuracy = accuracy_score(test_labels, test_preds)\nprecision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average='weighted')\nconf_matrix = confusion_matrix(test_labels, test_preds)\n\nprint(f\"Testing Accuracy: {test_accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\n# Plot confusion matrix\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Save model weights to a .pth file\noutput_dir = '/kaggle/working/model/'\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\ntorch.save(model.state_dict(), os.path.join(output_dir, 'fp_bert_model.pth'))\nprint(\"Model weights saved to:\", os.path.join(output_dir, 'fp_bert_model.pth'))\n","metadata":{},"execution_count":null,"outputs":[]}]}